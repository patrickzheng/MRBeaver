{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.json(\"s3n://patricks3db/reviews_Movies_and_TV_small.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check mrubash1/Origin for downloading and updating to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin',\n",
       " 'overall',\n",
       " 'reviewText',\n",
       " 'reviewerID',\n",
       " 'reviewerName',\n",
       " 'unixReviewTime']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = df\n",
    "ratings = ratings.drop(\"helpful\")\n",
    "#ratings = ratings.drop(\"reviewText\")\n",
    "ratings = ratings.drop(\"reviewTime\")\n",
    "#ratings = ratings.drop(\"reviewerName\")\n",
    "ratings = ratings.drop(\"summary\")\n",
    "#ratings = ratings.drop(\"unixReviewTime\")\n",
    "ratings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'affright',\n",
       " u'alarm',\n",
       " u'apprehension',\n",
       " u'apprehensiveness',\n",
       " u'boding',\n",
       " u'chill',\n",
       " u'consternation',\n",
       " u'creeps',\n",
       " u'diffidence',\n",
       " u'dismay',\n",
       " u'dread',\n",
       " u'enshrine',\n",
       " u'foreboding',\n",
       " u'freak',\n",
       " u'frisson',\n",
       " u'gloom',\n",
       " u'gloominess',\n",
       " u'hesitance',\n",
       " u'hesitancy',\n",
       " u'horror',\n",
       " u'hysteria',\n",
       " u'intimidation',\n",
       " u'pall',\n",
       " u'panic',\n",
       " u'premonition',\n",
       " u'presage',\n",
       " u'presentiment',\n",
       " u'quiver',\n",
       " u'saint',\n",
       " u'scare',\n",
       " u'shadow',\n",
       " u'shiver',\n",
       " u'shudder',\n",
       " u'shyness',\n",
       " u'somberness',\n",
       " u'sombreness',\n",
       " u'suspense',\n",
       " u'swivet',\n",
       " u'terror',\n",
       " u'thrill',\n",
       " u'timidity',\n",
       " u'timidness',\n",
       " u'timorousness',\n",
       " u'tingle',\n",
       " u'trepidation',\n",
       " u'unassertiveness',\n",
       " u'worship']"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "moodList = wn.synsets('fear')\n",
    "finalMoodList = [list(mood.closure(lambda s: s.hyponyms())) for mood in moodList]\n",
    "finalMoodList = [item.lemma_names() for moodList in finalMoodList for item in moodList]\n",
    "finalMoodList = [item for moodList in finalMoodList for item in moodList]\n",
    "pattern = re.compile(r'[A-Z]|\\_|\\-')\n",
    "finalMoodList = [elem for elem in finalMoodList if not pattern.findall(elem)]\n",
    "#finalMoodList = [list(mood.closure(lambda s: s.hypernyms())) for mood in moodList]\n",
    "finalMoodList = set(finalMoodList)\n",
    "finalMoodList = sorted(finalMoodList)\n",
    "finalMoodList\n",
    "#hypo = lambda s: s.hyponyms()\n",
    "#hyper = lambda s: s.hypernyms()\n",
    "#list(dog.closure(hypo, depth=1)) == dog.hyponyms()\n",
    "#list(dog.closure(hyper, depth=1)) == dog.hypernyms()\n",
    "#list(dog.closure(hypo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'play'"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.morphy('plays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testStr = \"I bought this for my husband who plays the piano.  He is having a wonderful time playing these old hymns.  The music  is at times hard to read because we think the book was published for singing from more than playing from.  Great purchase though!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "cnt = Counter()\n",
    "for word in tknzr.tokenize(testStr):\n",
    "    word = wn.morphy(word)\n",
    "    if word in finalMoodList: cnt[word] += 1\n",
    "    #cnt[word] += 1\n",
    "cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviewerSummaryRDD1 = rating.map(lambda row: (row.asin, row.reviewText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "          contains(sans) = True              neg : pos    =      8.2 : 1.0\n",
      "    contains(mediocrity) = True              neg : pos    =      7.5 : 1.0\n",
      "     contains(dismissed) = True              pos : neg    =      7.1 : 1.0\n",
      "   contains(overwhelmed) = True              pos : neg    =      6.5 : 1.0\n",
      "     contains(uplifting) = True              pos : neg    =      6.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "documents = [(list(movie_reviews.words(fileid)), category) \\\n",
    "    for category in movie_reviews.categories() \\\n",
    "    for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "#print(document_features(movie_reviews.words('pos/cv957_8737.txt'))) \n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#numRatings = ratings.count()\n",
    "#numRatings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#users = ratings.map(lambda r: r.reviewerID).distinct().zipWithIndex()\n",
    "#users.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#numUsers = users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#numUsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#numUsersPartitions = users.getNumPartitions()\n",
    "#userids = sc.parallelize(range(numUsers), numUsersPartitions)\n",
    "#usersWithID = users.zip(userids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#users = ratings.map(lambda r: r.reviewerID).distinct().zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#movies = ratings.map(lambda r: r.asin).distinct().zipWithIndex()\n",
    "#movies = ratings.map(lambda r: r.asin).distinct().collect()\n",
    "#movies = dict([(y,x) for x,y in enumerate(movies)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#moviesDf = sqlContext.createDataFrame(movies, ['asin', 'movieID'])\n",
    "#moviesDf.take(5)\n",
    "#moviesDf = sqlContext.createDataFrame(movies)\n",
    "#type(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usersDf = sqlContext.createDataFrame(users, ['reviewerID', 'userID'])\n",
    "#usersDf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rawRatings1 = ratings.join(usersDf, ratings.reviewerID == usersDf.reviewerID).drop(\"reviewerID\")\n",
    "#rawRatings1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rawRatings2 = rawRatings1.join(moviesDf, ratings.asin == moviesDf.asin).drop(\"asin\")\n",
    "#rawRatings2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import uuid\n",
    "#testAsin = 'A1KBT3D1O0LA7L'\n",
    "#testAsinInt = int(id('A1KBT3D1O0LA7L'))\n",
    "#testAsinInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rawRatings3 = rawRatings2.map(lambda row: (row.userID, row.movieID, row.overall))\n",
    "#ratings = ratings.map(lambda row: (users[row.reviewerID], movies[row.asin], row.overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rawRatings3.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rank = 10\n",
    "#numIterations = 10\n",
    "#model = ALS.train(rawRatings3, rank, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#type(ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#userMoviePair = usersDf.join(moviesDf).drop('reviewerID').drop('asin').map(lambda row: (row.userID, row.movieID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#userMoviePair.take(5)\n",
    "#type(userMoviePair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#recommendations = model.predictAll(userMoviePair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#type(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#recommendations.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
